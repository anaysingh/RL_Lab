{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4AgFDAdesoRnbhfq1JRwE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anaysingh/RL_Lab_21CSU011/blob/main/value_iteration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYUM-1LUxH8D",
        "outputId": "82a5da96-fc6e-4f8f-ff42-545ecdc597c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Value Function:\n",
            "State 0: 7.964879229580072\n",
            "State 1: 7.682260457853536\n",
            "State 2: 7.289734386011126\n",
            "State 3: 8.099734386011127\n",
            "State 4: 8.999734386011127\n",
            "State 5: 9.999734386011127\n",
            "\n",
            "Optimal Policy:\n",
            "State 0: Right\n",
            "State 1: Right\n",
            "State 2: Right\n",
            "State 3: Right\n",
            "State 4: Right\n",
            "State 5: Right\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the MDP as a tuple of states, actions, transition probabilities, rewards, and discount factor.\n",
        "states = [0, 1, 2, 3, 4, 5]\n",
        "actions = [0, 1]  # 0: Left, 1: Right\n",
        "num_states = len(states)\n",
        "num_actions = len(actions)\n",
        "\n",
        "# Transition probabilities (transition_probs[state][action] = [(next_state, probability)])\n",
        "transition_probs = [\n",
        "    {0: [(0, 0.9), (1, 0.1)], 1: [(1, 0.8), (0, 0.2)]},\n",
        "    {0: [(0, 0.1), (1, 0.9)], 1: [(2, 0.8), (0, 0.2)]},\n",
        "    {0: [(0, 0.0), (1, 0.0)], 1: [(3, 1.0)]},  # Updated transition probabilities\n",
        "    {0: [(0, 0.0), (1, 0.0)], 1: [(4, 1.0)]},  # Updated transition probabilities\n",
        "    {0: [(0, 0.0), (1, 0.0)], 1: [(5, 1.0)]},  # Updated transition probabilities\n",
        "    {0: [(0, 1.0)], 1: [(5, 1.0)]\n",
        "    }\n",
        "]\n",
        "\n",
        "# Rewards (rewards[state][action] = reward)\n",
        "rewards = [\n",
        "    {0: 0, 1: 1},\n",
        "    {0: 0, 1: 1},\n",
        "    {0: 0, 1: 0},  # Updated rewards\n",
        "    {0: 0, 1: 0},  # Updated rewards\n",
        "    {0: 0, 1: 0},  # Updated rewards\n",
        "    {0: 0, 1: 1}\n",
        "]\n",
        "\n",
        "# Discount factor\n",
        "gamma = 0.9\n",
        "\n",
        "# Initialize the value function\n",
        "V = np.zeros(num_states)\n",
        "\n",
        "# Value iteration\n",
        "num_iterations = 100\n",
        "for _ in range(num_iterations):\n",
        "    new_V = np.zeros(num_states)\n",
        "    for state in states:\n",
        "        max_action_value = float('-inf')\n",
        "        for action in actions:\n",
        "            action_value = 0\n",
        "            for next_state, prob in transition_probs[state][action]:\n",
        "                action_value += prob * (rewards[state][action] + gamma * V[next_state])\n",
        "            if action_value > max_action_value:\n",
        "                max_action_value = action_value\n",
        "        new_V[state] = max_action_value\n",
        "    V = new_V\n",
        "\n",
        "# Print the optimal value function\n",
        "print(\"Optimal Value Function:\")\n",
        "for state, value in enumerate(V):\n",
        "    print(f\"State {state}: {value}\")\n",
        "\n",
        "# Find the optimal policy\n",
        "policy = np.zeros(num_states, dtype=int)\n",
        "for state in states:\n",
        "    max_action_value = float('-inf')\n",
        "    for action in actions:\n",
        "        action_value = 0\n",
        "        for next_state, prob in transition_probs[state][action]:\n",
        "            action_value += prob * (rewards[state][action] + gamma * V[next_state])\n",
        "        if action_value > max_action_value:\n",
        "            max_action_value = action_value\n",
        "            policy[state] = action\n",
        "\n",
        "# Print the optimal policy\n",
        "print(\"\\nOptimal Policy:\")\n",
        "for state, action in enumerate(policy):\n",
        "    print(f\"State {state}: {'Left' if action == 0 else 'Right'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ai1IFGRyxKnc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}